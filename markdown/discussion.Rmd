## Discussion
We set out to expand the research on cognitive processes underlying post-error slowing (PES) using Bayesian hierarchical Drift Diffusion Modeling (DDM) and a systematic manipulation of the response-stimulus interval (RSI). We tried to investigate differences in the effect of error trials on response behavior depending on the time that passed since the error has occurred. For trials with short RSI (200ms), we mostly expected maladaptive processes to affect post-error behavior, leading to decreases in attention reflected by lower drift rates. On the other hand, for trials with long RSI (1000ms), maladaptive processes should have dissipated and adaptive changes in behavior should account for post-error slowing. Cognitive processes responsible for adaptive changes, such as increased response caution, should be reflected in higher boundary separation.

Our behavioral findings show that errors lead to greater accuracy reduction in the short RSI condition compared to the long RSI condition. This implies an enhanced involvement of maladaptive processes during the short RSI, which aligns with earlier studies [@dudschig2009; @jentzsch2009; @buzzel2017]. Post-error performance was compromised particularly in the case of a short RSI, as evidenced by both increased response time and reduced accuracy. The long RSI condition exhibited less pronounced negative effects on both response time and accuracy. When utilizing the robust approach to define PES, participants exhibited only a slight decrease in response time in the long RSI condition, while the classical approach actually indicates faster responses following an error. Even in the long RSI condition, reductions in accuracy post-error remained, indicating some lingering influence of maladaptive processes [@dudschig2009]. The magnitude of these decreases was less pronounced compared to the short RSI condition. This stronger impact of errors in the short RSI condition on accuracy in the following trials indicates that maladaptive processes influence response behavior especially in the short RSI condition.

The number of omissions following error trials provides additional evidence for an increased role of maladaptive processes at short RSI. In the short condition, participants failed to respond at all in `r 100*proportion_omissions_location %>% filter(rsi == 0.2, location == "E+1") %>% pull(freq)`% of trials following an error compared to an average omission rate of `r 100*proportion_omissions_location %>% filter(rsi == 0.2, location != "E+1") %>% pull(freq) %>% mean()` in all other trials. In the long condition, this rate was reduced to `r 100*proportion_omissions_location %>% filter(rsi == 1, location == "E+1") %>% pull(freq)`%. This pattern further supports an increased influence of maladaptive processes. Errors lead to slower response time likely due to _error-monitoring processes_ requiring some amount of central cognitive resources [@dudschig2009]. This leads to a high rate of non-responses before the response deadline. 
 
To further analyze the cognitive processes underlying post-error behavior we attempted to fit a Bayesian hierarchical DDM to data in both RSI conditions. However, we encountered problems when attempting to fit the model to data in the short RSI condition on post-error trials (E+1). These issues with fitting the data stem from the 800ms response time deadline censoring responses. Participants responded slower in the short RSI condition and slower following errors. This results in a considerable proportion of responses (`r 100*proportion_omissions_location %>% filter(rsi == 0.2, location == "E+1") %>% pull(freq)`%) being coded as omissions because participants did not respond before the deadline. These omissions generate the steep drop after the 800ms deadline in this condition's response time distribution (see Figure \@ref(fig:discussion-ep1-short-plot-rt-dist)). The basic DDM is not able to account for this censoring of response time distributions. Recently, researchers were able to extend the LBA model to accurately account for omissions of this type [@damaso2021]. Extending the DDM in a similar manner may result a better fit  to the present empirical data. It would also allow us to test our initial hypothesis by comparing parameter changes in long RSI conditions to changes in short RSI conditions. This can be accomplished by adding the number of omissions as a parameter that is estimated by an integral over the response time distribution between the deadline and infinity.
$$n_{omissions(ik)} = \int_{deadline}^{\infty} Wiener(a_{(ijk)}, z_{(ijk)}, Ter_{(ijk)}, v_{(ijk)})dT_{ijk}$$
However, developing and testing this extension would however exceed the scope of this body of work and will be deferred to future endeavors.

Due to the models inability to accurately fit the response time distribution on post-error trials in the short RSI condition, parameters estimated in this condition can not be interpreted. This prevents us from gaining any insight into differential effects of errors on post-error behavior depending on the RSI using DDM. 

```{r discussion-ep1-short-plot-rt-dist, cache = TRUE, warning = FALSE, fig.cap='Reponse time distribution on post-error trials in the short rsi conditions. All responses shorter than 150ms were excluded, the next trial began after 800ms after stimulus presentation.'}
rt_dist_ep1_short <- diffusion_data_location %>% 
  filter(location == "E+1", rsi == "short") %>% 
  ggplot(
    aes(x = rt)
  )+
  geom_histogram(bins = 55)+
  xlim(0, 1)+
  labs(
    x = "Response time (in s)",
    y = "count"
  )+
  papaja::theme_apa()

rt_dist_ep1_short
```

Parameters obtained from data originating in the long RSI condition do not show this misfit to the data. We expected to find evidence of adaptive processes at long RSI resulting in increased boundary separation. Maladaptive processes leading to attentional deficits following errors should lead to decreases in drift rate, which we mostly expected at short RSI. This decrease in drift rate should be smaller in long RSI.

In line with previous research [@dutilh2013; @purcell2016; @schiffler2017] we found post-error decreases in drift rate, even in the long RSI condition. Maladaptive processes such as decreased attention as posed by the orientating account [@notebaert2009] or reduced availability of central cognitive processing capabilities posited by @dudschig2009 seem to influence post-error behavior even after 1000ms.

We also observed decreases in non-decision time following errors, adding to the increasingly diverse findings regarding non-decision time increases [@dutilh2013] or decreases [@schiffler2017; @white2010; @dutilh2012testing] in trials following errors. In the non-hierarchical DDM we found reverse effects on non-decision time. This is likely due to the high reliance of non-decision time $T_{er}$ on the minimal response times in the estimated condition. For this reason, interpreting differences between conditions remains problematic [@singmann2018]. This inconsistency further strengthens the idea that completely pooling data should be avoided, as it may lead to misleading results.

Numerous previous studies [@white2010; @schiffler2017; @white2010; @dutilh2012testing] did however observe increases in boundary separation following errors. These findings support the idea that errors lead to increases in caution. We failed to find any evidence for increases such in caution following errors.

<!-- In contrast to all previous studies, only inspecting changes in boundary separation in our experiment does not accurately reflect changes in the amount of evidence needed to make a decision. In the present experiment the amount of evidence accumulated is strongly dependent on the bias, too. Shifts in starting point bias towards the previously not presented stimulus allow the model to capture participants expecting to see alternating stimuli. Observed decreases in bias following errors reflect an increase in the amount of evidence needed before making a decision. Even after accounting for this, the decrease in boundary separation following errors is strong enough to result in a net decrease in evidence needed before making a decision (see Figure \@ref(fig:discussion-evidence-needed-plot)). Participants accumulate less evidence before making a decision even after 1000ms. -->
<!-- Aside from the possible influence of confounds in experimental design, post-error adjustments remain the most likely factor influencing parameter changes in post-error trials in the long RSI condition. The reduction of boundary separation following errors should therefore also be discussed in terms of reflecting true adjustments in post-error behavior. One factor determining these changes is the type of error itself.  -->

Decreases in caution following errors were observed in only one previous study. @damaso2022 found evidence for decreased caution when applying a _Linear Ballistic Accumulator_ (LBA) model to data obtained by @osth2017 but observed no decreases in boundary separation when fitting a DDM to the same data. In their work, @damaso2022 differentiated between two types of errors, _evidence quality_ and _response speed_ errors as a potential explanation for decreases in caution [@damaso2020]. Evidence quality errors occur when participants are unsure of the correct response and default to guessing. Response speed errors on the other hand are due to participants responding too quickly. Had they waited for more evidence to accumulate, they would have made the correct choice. Post-error slowing in general and increases in caution more specifically only help eliminate further response speed errors. @damaso2022 infer from their findings of decreases in caution following errors, that those errors must mostly be evidence quality errors. An increase in boundary separation would not have helped participants improve their accuracy, so the decrease in boundary separation optimizes response speed while not affecting accuracy. However, the present task was explicitly chosen because it elicits mainly response speed errors. In our experiment, increases in caution help prevent further errors.

This feature of the present task may also lead to another explanation for post-error adjustments. The task is very simple to understand and the Go trials themselves are not particularly difficult. However, it is very difficult to inhibit a response during a NoGo trial. The simplicity of the task and seeming unavoidability of an error may lead to frustration following an error. @williams2016 coined the term _post-error recklessness_ to refer to a decrease in caution following errors due to increased frustration. Post-error recklessness is able to account for decreases in boundary separation following an error. This could be the case even in the long RSI condition, where long intervals in between trials may increase the frustration of a seemingly unavoidable error even further. 

The short deadline which lead to model issues in the short RSI condition may lead to another potential explanation for the lower boundary separation in post-error trials. Participants realize that they sometimes fail to respond, especially following error trials. This leads them to adapt their response behavior following an error trial in order to increase their response speed. Errors may increase the importance of response speed in the following trial. Emphasis on response speed has been shown to lead to a decrease in boundary separation [@voss2004; @ratcliff1998].

Our behavioral findings show a clear difference in the impact of errors on post-error behavior depending on the RSI. In the short RSI condition, accuracy decreases and response time increases were more pronounced than in the long condition, leading us to believe that maladaptive processes impact behavior more strongly in the short RSI condition. We encountered serious issues with model fit when attempting to fit the DDM to _post-error_ trial data in the short RSI condition, due to this conditions' high rate of omissions. Therefore, we only used the DDM to fit data stemming from the long RSI condition. Inspection of model parameters revealed a surprising decrease in caution following errors. We attribute this to effects of _post-error recklessness_. Frustrating errors lead participants to accumulate less evidence before making a decision. It is also plausible that decreases in boundary separation reflect participants speeding up their responses following errors aiming to not miss out on responding. This is caused by the short response deadline imposing a speed-emphasis on trials, especially those following an error. Participants may realize that they tend to respond slower in trials following an error, sometimes failing to respond at all. In response to this, they might adapt their boundary separation in order to speed up response processes. Our current experimental design only reveals this decrease in caution, but does not allow us to differentiate between _post-error recklessness_ and effects of the emphasis on response speed.
<!-- This is the result of averaging all participants' PES measures into one and neglects inter-individual differences in PES. We find considerable individual differences in PES in the long RSI condition. Some participants display _post-error speeding_ as predicted by post-error recklessness. The _complete pooling_ technique employed to ensure model fit is not able to account for these differences. A hierarchical model is better suited to properly account for individual differences in cognitive processes following errors^[We tried fitting a hierarchical model to this data and achieved good convergence criteria and good model fit, but disregarded the model due to a large number of divergent transitions. Model results are reported in Appendix **REPLACE**.]. -->

<!-- ### Pre-error speeding -->
<!-- A strength of our experimental design is the ability to discern pre-correct from pre-error trials, allowing us to study parameter differences resulting in _pre-error speeding_. We compared trials preceding correct responses to a NoGo trial to those preceding an error in a NoGo trial in both conditions. Behaviorally, response times were lower prior to an error, with this effect being more pronounced in the long RSI condition. This difference between RSI conditions has not been reported in previous research, leading researchers to believe that pre-error speeding is not the result of a strategic process [@dudschig2009]. -->

<!-- We were once again only able to fit a hierarchical model to data in the long RSI condition. The model that included data from the short RSI condition had acceptable $\widehat{R}$ and appropriate effective sample sizes, but a large number of divergent transitions. -->

<!-- In the model using data from the long RSI condition, DDM parameters reveal lower drift rates, lower boundary separation but increased non-decision time prior to error responses. The direction of parameter differences responsible for pre-error speeding is identical to those leading to post-error slowing. The key difference is the magnitude of changes. A larger decrease in drift rate following errors seems to induce the slight slowing, whereas boundary separation decreases in pre-error trials lead to faster responses. -->

<!-- Observed global shifts in performance prompted us to employ the robust approach to PES. This approach compares post-error trials only to those post-correct trials that occurred prior to an error. Post-error and pre-error trials are thus sampled from the same position in the data. Pre-error speeding calls the “robustness” of the robust approach into question. The robust approach is able to combat global performance shifts [@dutilh2012how] but fails to account for local performance shifts such as pre-error speeding [@pfister2022]. The present data supports the use of both robust and classical measures of PES when inspecting post-error behavior^[For the sake of brevity and due to model issues we chose not to report results of inspecting parameter differences using the classical approach to PES as they do not differ from those obtained using the robust method. Results obtained when fitting the DDM to data classified in a classical manner can be found in Appendix C.].  -->

### Limitations
One possible explanation for the inconsistent results obtained in this study might stem from problems in the study design. One could argue that participants do not accumulate evidence towards one of the decision boundaries "X" or "Y" but rather accumulate evidence towards "responding" or "not responding". This would violate a key assumption of the DDM as no true response selection takes place. Participants do not have to select between one of the response buttons when making a response, but only have to decided between responding and not responding. This strongly calls into question the results of fitting this version of the DDM model. Future work may attempt fitting the present data to a version of the DDM more equipped to deal with data stemming from Go-NoGo tasks [@ratcliff2018].

Additionally, as the task consists mostly of Go trials and the RSI is constant within a block, anticipation effects may play a large role. Participants anticipate having to give a Go response and begin their response process even before the trial has begun. The full DDM may be able to somewhat compensate this issue by introducing variability of the starting point as a new parameter and thus allowing for some trials to have starting points closer to a response boundary. However, high impact of anticipation on responses should lead to larger error rates in NoGo trials due to increased difficulty to inhibit responses when less evidence needs to be accumulated. Participants of our study had better performance on NoGo trials than participants in previous studies employing similar versions of this task [@hester2007], suggesting their performance was not worsened by anticipation effects.

Another potential issue is the confound of post-error and post-nogo behavior. All post-error trials are simultaneously post-NoGo trials whereas pre-error trials are always post-Go trials. A potential solution to this problem is to not only consider the impact of errors by investigating the difference between post-error and pre-error trials as a measure of PES ($\Delta_{PES} = \theta_{E+1} - \theta_{E-1}$), but also use the difference between post-correct and pre-correct trials ($\Delta_{nogo} = \theta_{C+1} - \theta_{C-1}$) as a baseline to partial out post-NoGo effects. However, falsely responding to a NoGo trial suggests that participants did not correctly process the NoGo trial. They also don't experience the long trial duration that accompanies a correct response inhibition in a NoGo trial. We therefore argue that responses following errors in NoGo trials are not impacted by "NoGo-effects" in the same way that responses following correct NoGo trials are. Therefore, we advise against controlling for post-NoGo effects using differences in trials surrounding correct responses to NoGo trials. Due to this flaw in the design, we are not able to fully rule out impacts of NoGo trials on post-error behavior.

One account predicting post-NoGo effects is that participants realize that most mistakes are committed in NoGo trials and begin to suspect that the likelihood of two consecutive NoGo trials is low. This leads to decreased boundary separation following NoGo trials due to the lowered likelihood of another NoGo trial appearing [@verbruggen2009]. However, we do not observe such decreases in boundary separation following correct responses to NoGo trials in the long RSI condition.

#### Model issues
In order to be comparable to previous research on PES using EAM [@purcell2016; @dutilh2012testing; @dutilh2013; @schiffler2017], this study focused on modeling response data using the DDM. Recent publications have however pointed to potential issues arising with fitting DDM to PES data and have suggested using LBA modeling [@brown2008] to avoid misfits [@damaso2022]. Future research will aim to compare DDM and LBA fitted to behavioral data and further investigate possible benefits and drawbacks of each mode of analysis. Fitting a LBA model may also help to resolve some of the issues described below.

Overall, multiple issues with fitting models to data in the short RSI condition prevented us from studying our initial hypothesis concerning the difference in post-error effects depending on the RSI. We attribute these issues to the large number of omissions present in post-error trials. However, the hierarchical models investigating pre-error speeding also had issues with large numbers of divergent transitions. 

Furthermore, a potential issue is the low overall number of trials ($< 100$) with inaccurate responses in Go trials. This leads to less valid estimates regarding the lower response boundary and most likely contributes to convergence issues, especially in a hierarchical setting where some participants might not have a single lower-boundary response. This could also explain the surprising finding that the starting point is biased towards an inaccurate response. We attribute this estimation of the starting point bias towards an inaccurate response as the models way of accounting for fast lower-boundary (error) response times. The low number of error trials might also render this parameter unreliable.

All in all, we are highly skeptical of the DDM's ability to fit the data in this specific task. The large number of issues when fitting models, as well as the potential violation of the basic assumption of evidence accumulation towards response boundaries, leaves us doubting the validity of the parameters estimated here. We suggest that future research employ a more suitable task with longer response deadlines or a more suitable cognitive model to combat the problems we faced. One approach might be extending the present task by removing the NoGo condition and instead having participants press a third button. This would turn the task into a very simple _1-back_ task. However, the simplicity of the task might reduce error rates further and lead to difficulties in sample size when applying complex cognitive models.

### Conclusion
**Fokus auf behaviorale Daten und future research**

Even in the long RSI condition, we failed to find evidence for adaptive processes in post-error trials. Errors lead to decreases in drift rate reflecting attentional deficits, increases in non-decision time and decreases in boundary separation reflecting post-error recklessness. Whether this influence of maladaptive processes would be increased in the short RSI condition as predicted by our hypothesis remains unclear due to problems fitting the DDM to data in this condition. We hope to remedy this issue in future research by extending the DDM to adequately account for omissions. Further limitations of this study include the inability of our experimental design to differentiate post-error from post-NoGo effects and the possible influence of anticipation effects. Additionally, we encountered issues when attempting to fit models to the short RSI condition and when attempting to fit more complex models. Due to the previously described issues, we strongly suggest using a different task to investigate post-error effects in the future. Overall, the validity of the parameters obtained in this study should be questioned.

The differences of the results obtained here compared to those obtained in previous studies illustrate the importance of considering the specifics of the task involved when studying PES. We also suggest evaluating PES in both the classical and robust approach, as neither one can account for both local and global performance shifts. Finally, our failure to find adaptive effects following error responses in this task should discourage further researches from using PES as a direct measure of cognitive control.

<!-- Size and nature of PES can differ depending on the task, RSI, participants, error type, definition of PES and presence of feedback. **citations here** -->

<!-- NOTES ------------- -->

<!-- Report results but strongly advise against using them, explain them in terms of post-error frustration or post-error recklessness -->
<!-- here is why -->
<!-- bad fit to empirical data -->
<!-- - truncated responses at short e+1 mostly (40% missing data) -->
<!-- - anticipation effects -->
<!-- - multiple post-error processes -->
<!-- - ideal of "more cautious behavior" might not be real. Affective components always play a role -->
<!-- - sampling (non-omitted) trials, mostly trials that have lower boundary separation -->

<!-- - participants not working on the task properly? accuracy from nogo trials suggests differently -->

<!-- - run a model with median split trial number to investigate fatigue effects? -->

<!-- - but these problems don't seem to play a role in long RSI, so findings there are still valid.  -->
<!-- **Error rate long/short is not an issue for "orienting account"** -->

<!-- Check data for hints of classic/robust accounts being better? Can we find "global shifts" in parameters, or are differences C/E Location only "phases" and time specific to errors. -->

<!-- Check stop-signal task results, show that you could account for post-nogo effects using the anova approach -->
<!-- -describe theory -->
<!-- -describe what was done -->
<!-- - task, PES spec, model -->
<!-- - describe results -->
<!-- - say what that means theoretically, show what accounts it contributes with -->
<!-- - boundary separation last -->
<!-- - discuss potential reasons for this -->
<!-- - discuss how Damaso 2022 found same thing (Lex Task, with Confidence ratings, huge RSI) -->
<!-- - Might be due to confounding problems -->
<!-- - Might be due to errors being post-NoGo - that would only show in "robust" comparison -->
<!-- 	- Find some evidence of post-Nogo effects? -->
<!-- - Might be due to errors being post-response - that would only show in "classic" condition -->
<!-- 	- Find some evidence of post-inhibition response? -->
<!-- - Compare post-NoGo to post-Go (C-1 -- C+1) to fix first or C+1 -- C-1 -->
<!-- - Ultimately, you cannot separate a response following an error from also following a failed inhibition trial -->
<!-- - Maybe compare effects of Go-errors? Don't have any go-errors (practically) that aren't NA responses -->

<!-- might be due to real decreases in boundary separation -->
<!-- - talk about error types -->
<!-- - talk about post-error recklessness -->
<!-- 	- Damaso et al (2020) say post-error recklessness, lending to post-error speeding is found mainly after evidence-quality errors -->


<!-- ## further research -->
<!-- Better task! This one allowed the location model, but it's also shit -->
<!-- Investigate group-level sd in E-1 compared to E+1, more variance following an error? -> individual differences in error processing. Tentative suggestions at best -->
<!-- Just talk about the influence of tasks, error types and response instructions more generally. It seems to make a huge difference! -->

<!-- Speed and accuracy instruction seems to matter (Damaso, 2022), maybe take a look at whether individuals preferred speed/accuracy? Or manipulate that in later research -->

<!-- ## Limitations -->
<!-- Discuss model fit here -->
<!-- Discuss the paper suggesting LBA modelling here -->
<!-- - Damaso et al. suggest DDM bad because it doesn't separate error and correct responses -> bring up the error rate in trials analysed.  -->
<!-- - max error rate in my experiment is 5% - 480/7200 trials errors. (E+1) -->
<!-- Error types: Do I find evidence for response quality errors? Percent of error responses to nogo-trials slower than correct responses? Somewhat pointless because of inhibition. -->


<!-- Maybe say: Effect of error on rt not generalizable to "errors" in general but more specific to inhibition errors in nogo trials. Should be on the safe side with that. -->

<!-- Effect of error should depend on a) requirements of task and b) the process that failed -->
<!-- - maybe find some evidence of that -->