# Discussion
We set out to expand the study of cognitive processes underlying post-error slowing (PES) with the help of Drift Diffusion Modeling (DDM) and a systematic manipulation of the response-stimulus interval (RSI). We tried to investigate differences in the effect of error trials on response behavior depending on the time that has passed since the error has occurred. When the RSI is short (200ms) we expected mostly maladaptive processes to affect post-error behavior, leading to decreases in attention reflected by lower drift rates. Is the RSI long on the other hand, maladaptive processes should have dissipated and adaptive changes in behavior such as increased response caution reflected in higher boundary separation should account for post-error slowing.
 
Due to the models inability to accurately fit the response time distribution on post-error trials in the short RSI condition, parameters estimated in this condition are not interpretable. This prevents us from gaining any insight into differential effects of errors on post-error behavior depending on the RSI. 

Issues with fitting the data most likely stem from the 800ms response time limit censoring responses of participants. A considerable proportion of responses (`r 100*proportion_omissions_location %>% filter(rsi == 0.2, location == "E+1") %>% pull(freq)`%) were coded as omissions because participants did not respond before the deadline. This results in the steep drop after the 800ms deadline found when investigating this conditions' response time distribution (see Figure \@ref(fig:discussion-ep1-short-plot-rt-dist)). The basic DDM is not able to account for this censoring of response time distributions. Recently, researchers were able to extend the LBA model to accurately account for omissions of this type [@damaso2021]. Extending the DDM in a similar manner may result in it being able to fit the present empirical data and will allow us to test our initial hypothesis by comparing parameter changes in long RSI conditions to changes in short RSI conditions. Developing and testing this extension would however exceed the scope of this body of work and will be relegated to future endeavors.

```{r discussion-ep1-short-plot-rt-dist, cache = TRUE, warning = FALSE, fig.cap='Reponse time distribution on post-error trials in the short rsi conditions. All responses shorter than 150ms were excluded, the next trial began after 800ms after stimulus presentation.'}
rt_dist_ep1_short <- diffusion_data_location %>% 
  filter(location == "E+1", rsi == "short") %>% 
  ggplot(
    aes(x = rt)
  )+
  geom_histogram(bins = 55)+
  xlim(0, 1)+
  labs(
    x = "Response time (in s)",
    y = "count"
  )+
  papaja::theme_apa()

rt_dist_ep1_short
```

Parameters obtained from data originating in the long RSI condition do not show this extreme misfit to the data. We expected to find evidence of adaptive processes at long RSI resulting in increased boundary separation. Maladaptive processes leading to attentional deficits following errors should lead to decreases in drift rate, which we mostly expected at short RSI. This decrease in drift rate should be smaller in long RSI.

In line with previous research [@dutilh2013; @purcell2016; @schiffler2017] we found post-error decreases in drift rate, even in the long RSI. Maladaptive processes such decreased attention as posed by the orientating account [@notebaert2009] or reduced availability of central cognitive processing capabilities posited by @dudschig2009 seem to influence post-error behavior even after 1000ms.

We also observed increases in non-decision time following errors, adding to the increasingly diverse findings regarding non-decision time increases [@dutilh2013] or decreases [@schiffler2017; @white2010; @dutilh2012testing] in trials following errors.

All those previous studies did however observe increases in boundary separation following errors. These findings supported the idea that errors lead to some attentional deficit that is  offset by increases in caution [@schiffler2017]. In our data, we failed to find any evidence for increases in caution following errors.

<!-- In contrast to all previous studies, only inspecting changes in boundary separation in our experiment does not accurately reflect changes in the amount of evidence needed to make a decision. In the present experiment the amount of evidence accumulated is strongly dependent on the bias, too. Shifts in starting point bias towards the previously not presented stimulus allow the model to capture participants expecting to see alternating stimuli. Observed decreases in bias following errors reflect an increase in the amount of evidence needed before making a decision. Even after accounting for this, the decrease in boundary separation following errors is strong enough to result in a net decrease in evidence needed before making a decision (see Figure \@ref(fig:discussion-evidence-needed-plot)). Participants accumulate less evidence before making a decision even after 1000ms. -->
<!-- Aside from the possible influence of confounds in experimental design, post-error adjustments remain the most likely factor influencing parameter changes in post-error trials in the long RSI condition. The reduction of boundary separation following errors should therefore also be discussed in terms of reflecting true adjustments in post-error behavior. One factor determining these changes is the type of error itself.  -->

Decreases in caution following errors were observed in one previous study. @damaso2022 found evidence for decreased caution when applying an LBA model to data obtained by @osth2017 but observed no decreases in boundary separation when fitting a DDM to the same data. In their work, @damaso2022 diffentiated between two types of errors, _evidence quality_ and _response speed_ errors as a potential explanation for decreases in caution [@damaso2020]. Evidence quality errors occur when participants are unsure of the correct response and default to guessing. Response speed errors on the other hand are due to participants responding too quickly. Had they waited for more evidence to accumulate, they would have made the correct choice. Post-error slowing in general and increases in caution more specifically only help eliminate further response speed errors. @damaso2022 attribute their findings of decreases in caution following errors to the errors being mostly evidence quality errors. An increase in boundary separation would not have helped participants improve their accuracy, so the decrease in boundary separation optimizes response speed while not affecting accuracy. The present task however was explicitly chosen because it elicits mainly response speed errors, increases in caution do help prevent further errors.

This feature of the present task may also lead to another explanation of post-error adjustments found here. The task is very simple to understand and each Go trial is not particularly difficult. It is however very difficult to inhibit a response during a NoGo trial. The simplicity of the task and seeming unavoidability of an error may lead to frustration following an error. @williams2016 coined the term _post-error recklessness_ to refer to a decrease in caution following errors due to increased frustration. Post-error recklessness is able to account for decreases in boundary separation following an error. This could be the case even in the long RSI condition, where long intervals in between trials may increase the frustration of a seemingly unavoidable error even further. 

In our data we did not observe a decrease in response times following errors predicted by _post-error recklessness_ but found small PES even in the long RSI condition, this is due to the decrease in drift rate following error trials. Errors seem to induce maladaptive decreases in drift rate post-error even in the long RSI conditions. Participants display some symptoms of _post-error recklessness_ following errors, reflected in decreased boundary separation, but the decreases in drift rate prevent post-error speeding. Non-decision time is slightly increased following an error leading to further increases in response times.

<!-- This is the result of averaging all participants' PES measures into one and neglects inter-individual differences in PES. We find considerable individual differences in PES in the long RSI condition. Some participants display _post-error speeding_ as predicted by post-error recklessness. The _complete pooling_ technique employed to ensure model fit is not able to account for these differences. A hierarchical model is better suited to properly account for individual differences in cognitive processes following errors^[We tried fitting a hierarchical model to this data and achieved good convergence criteria and good model fit, but disregarded the model due to a large number of divergent transitions. Model results are reported in Appendix **REPLACE**.]. -->

## The case of pre-error speeding
A strength of our experimental design is the ability to discern pre-correct from pre-error trials, allowing us to study parameter differences resulting in _pre-error speeding_. We compared trials preceding correct responses to a NoGo trial to those preceding an error in a NoGo trial in both conditions. Behaviorally, response times were lower prior to an error, with this effect being more pronounced in the long RSI condition. This difference between RSI conditions was previously not found, leading researchers to believe that pre-error speeding is not the result of a strategic process [@dudschig2009].

Sadly, we were once again only able to fit a hierarchical model to data in the long RSI condition. The model including data from the short RSI condition had acceptable $\widehat{R}$ and good effective sample sizes, but a large number of divergent transitions.

DDM parameters reveal lower drift rates, lower boundary separation but increased non-decision time prior to error responses. The direction of parameter differences responsible for pre-error speeding is identical to those leading to post-error slowing. The key difference is the magnitude of changes. A larger decrease in drift rate following errors seems to induce the small slowing, whereas a boundary separation decreases pre-error lead to faster responses.

Observed global shifts in performance prompted us employ the robust definition, but pre-error speeding calls the “robustness” of the robust approach into question. The robust definition is able to combat global performance shifts [@dutilh2012how] but fails to account for local performance shifts such as pre-error speeding [@pfister2022]. The present data further shows that both robust and classical measures of PES should be used when inspecting post-error behavior^[For the sake of brevity and due to model issues we chose not to report results of inspecting parameter differences using the classical approach to PES as they do not differ from those obtained using the robust result. Results obtained when fitting the DDM to data classified in a classical manner can be found in Appendix C.]. 

## Limitations
One possible explanation for the inconsistent results obtained here might stem from problems in study design. The chosen task is extremely simple and does not require much evidence accumulation to take place. As such, accuracy in Go trials is very high. One might argue that participants do not accumulate evidence towards one of the decision boundaries "X" or "Y" but rather accumulate evidence towards "responding" or "not responding". This would violate the assumptions of the model specification employed here and calls in question the results of fitting this model.

Additionally, as the task is mostly consistent of Go trials and the RSI is constant in within a block, anticipation effects may play a large role. Participants anticipate having to give a Go response and begin their response process even before the trial has begun. The full DDM may be able to somewhat compensate for this issue by introducing variability of the starting point as a new parameter and thus allowing for some trials to have starting points closer to a response boundary. High impact of anticipation on responses should however lead to larger error rates in NoGo trials due to increased difficulty to inhibit responses when less evidence needs to be accumulated. Participants of our study had better performance on NoGo trials than participants in previous studies employing similar versions of this task [@hester2007], suggesting their performance was not worsened by large impacts of anticipation. We also observed good fit of predicted response time distributions even in conditions with anticipation-induced "bumps", leading us to believe that the impact of anticipation was at least somewhat captured by our model.

Another potential issue is the confound of post-error and post-nogo behavior. All post-error trials are simultaneously post-NoGo trials whereas pre-error trials are always post-Go trials. A potential solution to this problem is to not only consider the impact of errors by investigating the difference $\overline{RT_{E+1}} - \overline{RT_{E-1}}$ but also use the difference $\overline{RT_{C+1}} - \overline{RT_{C-1}}$ as a baseline to partial out post-NoGo effects. However, falsely responding to a NoGo trial suggests that participants did not correctly process the NoGo trial. They also don't experience the long trial duration that accompanies a correct response inhibition in a NoGo trial. We therefore argue that responses following errors in NoGo trials are not impacted by "NoGo-effects" in the same way that responses following correct NoGo trials are. This leads us to advise against controlling for post-NoGo effects using differences in trials surrounding correct responses to NoGo trials. Due to this flaw in the design, we are not able to fully rule out impacts of NoGo trials on post-error behavior.

One account predicting post-NoGo effects is that participants realize that most mistakes are committed in NoGo trials and begin to suspect that the likelihood of two consecutive NoGo trials is low. This leads to decreased boundary separation following NoGo trials due to a lower likelihood of another NoGo trial appearing [@verbruggen2009]. We do not observe decreases in boundary separation following correct responses to NoGo trials in the long RSI condition however, which this account would predict.

### Model issues
Issues with fitting models to data in the short RSI condition prevented us from studying our initial hypothesis concerning the difference in post-error effects depending on the RSI. We attribute these issues to the large number of omissions present in post-error trials. However, hierarchical models investigating pre-error speeding also had issues with large numbers of divergent transitions. 

Furthermore, a potential issue is the low number of trials ($< 100$) with inaccurate responses in NoGo trials. This leads to unreliable estimates regarding the lower response boundary and most likely contributes to convergence issues, especially in a hierarchical setting where some participants might not have a single lower-boundary response. We attribute the puzzling estimation of starting point bias towards an inaccurate response as the models way of accounting for fast error times. The low number of error trials might also render this parameter unreliable.

All in all, we are highly skeptical of the DDM's ability to fit the data in this specific task. The large number of issues when fitting models as well as the potential violation of the basic assumption of evidence accumulation towards response boundaries leaves us doubting the validity of the parameters estimated here. We suggest future research employ a more suitable task with longer response deadlines to combat the problems we faced.

## Conclusion
Even in the long RSI condition, we failed to find evidence for adaptive processes in post-error trials. Errors lead to decreases in drift rate reflecting attentional deficits, increases in non-decision time and decreases in boundary separation reflecting post-error recklessness. This decrease is only partially offset by a decrease in bias leaving a net decrease in evidence needed to reach a response boundary. Whether this influence of maladaptive processes would be increased in the short RSI condition as predicted by our hypothesis remains unclear due to problems fitting the model to data in this condition. We hope to remedy this issue in the future by extending the DDM to adequately account for omissions. Further limitations of this study include the inability of our experimental design to dissect post-error from post-NoGo effects and the possible influence of anticipation effects. Additionally, we encountered immense issues when attempting to fit models in the short RSI condition and when attempting to fit more complex models. We strongly suggest using a different task to investigate post-error effects in the future as the present task seems to induce issues when fitting a DDM. This leads us to question the validity of the parameters obtained in this study.

The differences in results obtained here to those obtained in previous studies illustrate the importance of considering the specifics of the task involved when studying PES. We also suggest evaluating PES in both the classical and robust approach, as neither one can account for both local and global performance shifts. Finally, our failure to find adaptive effects following error responses in this task should warn further researches to use PES as a direct measure of cognitive control.

<!-- Size and nature of PES can differ depending on the task, RSI, participants, error type, definition of PES and presence of feedback. **citations here** -->

<!-- NOTES ------------- -->

<!-- Report results but strongly advise against using them, explain them in terms of post-error frustration or post-error recklessness -->
<!-- here is why -->
<!-- bad fit to empirical data -->
<!-- - truncated responses at short e+1 mostly (40% missing data) -->
<!-- - anticipation effects -->
<!-- - multiple post-error processes -->
<!-- - ideal of "more cautious behavior" might not be real. Affective components always play a role -->
<!-- - sampling (non-omitted) trials, mostly trials that have lower boundary separation -->

<!-- - participants not working on the task properly? accuracy from nogo trials suggests differently -->

<!-- - run a model with median split trial number to investigate fatigue effects? -->

<!-- - but these problems don't seem to play a role in long RSI, so findings there are still valid.  -->
<!-- **Error rate long/short is not an issue for "orienting account"** -->

<!-- Check data for hints of classic/robust accounts being better? Can we find "global shifts" in parameters, or are differences C/E Location only "phases" and time specific to errors. -->

<!-- Check stop-signal task results, show that you could account for post-nogo effects using the anova approach -->
<!-- -describe theory -->
<!-- -describe what was done -->
<!-- - task, PES spec, model -->
<!-- - describe results -->
<!-- - say what that means theoretically, show what accounts it contributes with -->
<!-- - boundary separation last -->
<!-- - discuss potential reasons for this -->
<!-- - discuss how Damaso 2022 found same thing (Lex Task, with Confidence ratings, huge RSI) -->
<!-- - Might be due to confounding problems -->
<!-- - Might be due to errors being post-NoGo - that would only show in "robust" comparison -->
<!-- 	- Find some evidence of post-Nogo effects? -->
<!-- - Might be due to errors being post-response - that would only show in "classic" condition -->
<!-- 	- Find some evidence of post-inhibition response? -->
<!-- - Compare post-NoGo to post-Go (C-1 -- C+1) to fix first or C+1 -- C-1 -->
<!-- - Ultimately, you cannot separate a response following an error from also following a failed inhibition trial -->
<!-- - Maybe compare effects of Go-errors? Don't have any go-errors (practically) that aren't NA responses -->

<!-- might be due to real decreases in boundary separation -->
<!-- - talk about error types -->
<!-- - talk about post-error recklessness -->
<!-- 	- Damaso et al (2020) say post-error recklessness, lending to post-error speeding is found mainly after evidence-quality errors -->


<!-- ## further research -->
<!-- Better task! This one allowed the location model, but it's also shit -->
<!-- Investigate group-level sd in E-1 compared to E+1, more variance following an error? -> individual differences in error processing. Tentative suggestions at best -->
<!-- Just talk about the influence of tasks, error types and response instructions more generally. It seems to make a huge difference! -->

<!-- Speed and accuracy instruction seems to matter (Damaso, 2022), maybe take a look at whether individuals preferred speed/accuracy? Or manipulate that in later research -->

<!-- ## Limitations -->
<!-- Discuss model fit here -->
<!-- Discuss the paper suggesting LBA modelling here -->
<!-- - Damaso et al. suggest DDM bad because it doesn't separate error and correct responses -> bring up the error rate in trials analysed.  -->
<!-- - max error rate in my experiment is 5% - 480/7200 trials errors. (E+1) -->
<!-- Error types: Do I find evidence for response quality errors? Percent of error responses to nogo-trials slower than correct responses? Somewhat pointless because of inhibition. -->


<!-- Maybe say: Effect of error on rt not generalizable to "errors" in general but more specific to inhibition errors in nogo trials. Should be on the safe side with that. -->

<!-- Effect of error should depend on a) requirements of task and b) the process that failed -->
<!-- - maybe find some evidence of that -->