# Discussion
We set out to expand the study of cognitive processes underlying post-error slowing (PES) with the help of Drift Diffusion Modeling (DDM) and a systematic manipulation of the response-stimulus interval (RSI). We tried to investigate differences in the effect of error trials on response behavior depending on the time that has passed since the error has occurred. When the RSI is short (200ms) we expected mostly maladaptive processes to affect post-error behavior, leading to decreases in attention reflected by lower drift rates. Is the RSI long on the other hand, maladaptive processes should have dissipated and adaptive changes in behavior such as increased response caution reflected in higher boundary separation should account for post-error slowing.
 
Due to the models inability to accurately fit the response time distribution on post-error trials in the short RSI condition, parameters estimated in this condition are not interpretable. This prevents us from gaining any insight into differential effects of errors on post-error behavior depending on the RSI. 

Issues with fitting the data most likely stem from the 800ms response time limit censoring responses of participants. A considerable proportion of responses (`r 100*proportion_omissions_location %>% filter(rsi == 0.2, location == "E+1") %>% pull(freq)`%) were coded as omissions because participants did not respond before the deadline. This results in the steep drop after the 800ms deadline found when investigating this conditions' response time distribution (see Figure \@ref(fig:discussion-ep1-short-plot-rt-dist)). The basic DDM is not able to account for this censoring of response time distributions. Recently, researchers were able to extend the LBA model to accurately account for omissions of this type [@damaso2021]. Extending the DDM in this way may result in it being able to fit the present empirical data and will allow us to test our initial hypothesis by comparing parameter changes in long RSI conditions to changes in short RSI conditions. Developing and testing this extension would however exceed the scope of this body of work and will be relegated to future endeavors.

```{r discussion-ep1-short-plot-rt-dist, cache = TRUE, warning = FALSE, fig.cap='Reponse time distribution on post-error trials in the short rsi conditions. All responses shorter than 150ms were excluded, the next trial began after 800ms after stimulus presentation.'}
rt_dist_ep1_short <- diffusion_data_location %>% 
  filter(location == "E+1", rsi == "short") %>% 
  ggplot(
    aes(x = rt)
  )+
  geom_histogram(bins = 55)+
  xlim(0, 1)+
  labs(
    x = "Response time (in s)",
    y = "count"
  )+
  papaja::theme_apa()

rt_dist_ep1_short
```

Parameters obtained from data originating in the long RSI condition do not show this extreme misfit to the data. We expected to find evidence of adaptive processes at long RSI resulting in increased boundary separation. Maladaptive processes leading to attentional deficits following errors should lead to decreases in drift rate, which we mostly expected at short RSI. This decrease in drift rate should be smaller in long RSI.

In line with previous research [@dutilh2013; @purcell2016; @schiffler2017] we found post-error decreases in drift rate, even in the long RSI. Maladaptive processes such decreased attention as posed by the orientating account [@notebaert2009] or reduced availability of central cognitive processing capabilities posited by @dudschig2009 seem to influence post-error behavior even after 1000ms.

All those previous studies did however observe increases in boundary separation following errors. These findings supported the idea that errors lead to some attentional deficit that is  offset by increases in caution [@schiffler2017]. In our data, we failed to find any evidence for increases in boundary separation following errors. Decreases in boundary separation are not predicted by any accounts of PES and were only found in one other study [@damaso2022]. @damaso2022 found decreases in caution applying an LBA model to data obtained by @osth2017 but observed no decreases in boundary separation when fitting a DDM to the same data.

One possible explanation for the inconsistent results obtained here might stem from problems in study design. The chosen task is extremely simple and does not require much evidence accumulation to take place. As such, accuracies in Go trials are very high. One might argue that participants do not accumulate evidence towards one of the decision boundaries "X" or "Y" but rather accumulate evidence towards "responding" or "not responding". This would violate the assumptions of the model specification employed here and calls in question the results of fitting this model.

Additionally, as the task is mostly consistent of Go trials and the RSI is constant in within a block, anticipation effects may play a large role. Participants anticipate having to give a Go response and begin their response process even before the trial has begun. The full DDM may be able to somewhat compensate for this issue by introducing variability of the starting point as a new parameter and thus allowing for some trials to have starting points closer to a response boundary. High impact of anticipation on responses should however lead to larger error rates in NoGo trials due to increased difficulty to inhibit responses when less evidence needs to be accumulated. Participants of our study had better performance on NoGo trials than participants in previous studies employing similar versions of this task [@hester2007], suggesting their performance was not worsened by large impacts of anticipation. We also observed good fit of predicted response time distributions even in conditions with anticipation-induced "bumps", leading us to believe that the impact of anticipation was at least somewhat captured by our model.

Another potential issue is the confound of post-error and post-nogo behavior. All post-error trials are simultaneously post-NoGo trials whereas pre-error trials are always post-Go trials. A potential solution to this problem is to not only consider the impact of errors by investigating the difference $\overline{RT_{E+1}} - \overline{RT_{E-1}}$ but also use the difference $\overline{RT_{C+1}} - \overline{RT_{C-1}}$ as a baseline to partial out post-NoGo effects. However, falsely responding to a NoGo trial suggests that participants did not correctly process the NoGo trial. They also don't experience the long trial duration that accompanies a correct response inhibition in a NoGo trial. We therefore argue that responses following errors in NoGo trials are not impacted by "NoGo-effects" in the same way that responses following correct NoGo trials are. This leads us to advise against controlling for post-NoGo effects using differences in trials surrounding correct responses to NoGo trials. Due to this flaw in the design, we are not able to fully rule out impacts of NoGo trials on post-error behavior.

One account predicting post-NoGo effects is that participants realize that most mistakes are committed in NoGo trials and begin to suspect that the likelihood of two consecutive NoGo trials is low. This leads to decreased boundary separation following NoGo trials due to a lower likelihood of another NoGo trial appearing [@verbruggen2009]. We do not observe decreases in boundary separation following correct responses to NoGo trials in the long RSI condition however, which this account would predict. 

Post-error adjustments remain the most likely factor influencing parameter changes in post-error trials in the long RSI condition. One factor determining these changes is the type of error itself. **add connector here!** **something like: parameter changes may reflect changes in cognitive processes and not just an artifact of study flaws** @damaso2020 differentiate between two types of errors, _evidence quality_ and _response speed_ errors. Evidence quality errors occur when participants are unsure of the correct response and default to guessing. Response speed errors on the other hand are due to participants responding too quickly. Had they waited for more evidence to accumulate, they would have made the correct choice. Post-error slowing in general and increases in caution more specifically only help eliminate further response speed errors. @damaso2022 attribute their findings of decreases in caution following errors to the errors being mostly evidence quality errors. An increase in boundary separation would not have helped participants improve their accuracy, so the decrease in boundary separation optimizes response speed while not affecting accuracy. The present task however was explicitly chosen because it elicits mainly response speed errors, increases in caution do help prevent further errors.

This feature of the present task may also lead to another explanation of post-error adjustments found here. The task is very simple to understand and each Go trial is not particularly difficult. It is however very difficult to inhibit a response during a NoGo trial. The simplicity of the task and seeming unavoidability of an error may lead to frustration following an error. @williams2016 coined the term _post-error recklessness_ to refer to a decrease in caution following errors due to increased frustration. Post-error recklessness is able to account for decreases in boundary separation following an error. This could be the case even in the long RSI condition, where long intervals in between trials may increase the frustration of a seemingly unavoidable error even further.

Only inspecting changes in boundary separation does not accurately reflect changes in the amount of evidence needed to make a decision. In the present experiment the amount of evidence accumulated is strongly dependent on the bias, too. Shifts in starting point bias towards the previously not presented stimulus allow the model to capture participants' expecting to see alternating stimuli. Observed decreases in bias following errors reflect an increase in the amount of evidence needed before making a decision. After accounting for this, the decrease in boundary separation following errors is strong enough to result in a net decrease in evidence needed before making a decision (see Figure \@ref(fig:discussion-evidence-needed-plot)). Participants accumulate less evidence before making a decision even after 1000ms.

```{r discussion-evidence-needed-plot, fig.cap='Distance in evidence units from the start point to the nearest boundary in the long RSI condition'}
posterior_comp_evidence_accumulated_long
```

## pre-error speeding
A strength of our experimental design is the ability to discern pre-correct from pre-error trials, this allows us to study parameter differences resulting in _pre-error speeding_. Here, we were able to evaluate the short RSI condition, since no misfit in the predicted response time distribution was observed. We compared trials preceding correct responses to a NoGo trial to those preceding an error in a NoGo trial in both conditions. Response times were lower prior to an error, with this effect being more pronounced in the long RSI condition. This difference between RSI conditions was previously not found, leading researchers to believe that pre-error speeding is not the result of a strategic process [@dudschig2009].

DDM parameters reveal lower drift rates and lower boundary separation prior to error responses in both conditions. In the short RSI condition, reduced bias and increased non-decision time mitigate speeding whereas response time is further reduced by decreased non-decision time and increased bias in the long RSI condition. In both conditions, the amount of evidence needed to reach a decision boundary is lower prior to an error than prior to a correct response. Decreases in caution thus explain pre-error speeding in both conditions. 

In the short condition, this is mostly driven by decreases in boundary separation, suggesting participants accumulate less evidence overall before making a decision. In the long condition however, this global decrease in boundary separation is less pronounced and speeding is induced by a more specific shift in bias towards the previously not shown response. This suggests that participants speed up in the short condition because they become less cautious globally, whereas they speed up in the long condition because their behavior adapts to the alternating nature of the task. We find this indicative of differences in cognitive processes underlying pre-error speeding at different RSI. When RSI is short, participants become less cautious due to needing less evidence overall before making a decision. In blocks where participants have increased time to adapt their behavior, pre-error speeding is influenced by a shift of bias adapting to the alternating nature of the task.

These findings regarding pre-error speeding call the "robustness" of the robust approach into question. Global shifts in performance prompted us employ the robust definition. The robust definition is able to combat global performance shifts [@dutilh2012how] but fails to account for local performance shifts such as pre-error speeding [@pfister2022]. The present data further shows that both robust and classical measures of PES should be used when inspecting post-error behavior^[For the sake of brevity we chose not to report results of inspecting parameter differences using the classical approach to PES as they do not differ from those obtained using the robust result. Results obtained when fitting the DDM to data classified in a classical manner can be found in Appendix **REPLACE**.].

## Conclusion
Even in the long RSI condition, we failed to find evidence for adaptive processes in post-error trials. Errors seem to have mostly maladaptive consequences on post-error behavior as indicated by the important role of drift-rate decreases when investigating model comparisons. Whether this influence of maladaptive processes would be increased in short RSI conditions as predicted by our hypothesis remains unclear due to problems fitting the model to data in this condition. We hope to remedy this issue in the future by extending the DDM to adequately account for omissions. Further limitations of this study include the inability of our experimental design to dissect post-error from post-NoGo effects and anticipation effects.

Our data further underline the importance of evaluating PES in both the classical and robust approach, as neither one can account for both local and global performance shifts. The differences in results obtained here to those obtained in previous studies further illustrate the importance of considering the specifics of thetask involved when studying PES. Our failure to find adaptive effects following error responses in this task should warn further researches to use PES as a direct measure of cognitive control. Size and nature of PES can differ depending on the task, RSI, participants, error type, definition of PES and presence of feedback. **citations here**

<!-- NOTES ------------- -->

<!-- Report results but strongly advise against using them, explain them in terms of post-error frustration or post-error recklessnes -->
<!-- here is why -->
<!-- bad fit to empricial data -->
<!-- - truncated responses at short e+1 mostly (40% missing data) -->
<!-- - anticipation effects -->
<!-- - multiple post-error processes -->
<!-- - ideal of "more cautios behavior" might not be real. Affective components always play a role -->
<!-- - sampling (non-omitted) trials, mostly trials that have lower boundary separation -->

<!-- - participants not working on the task properly? accuracy from nogo trials suggests differently -->

<!-- - run a model with median split trial number to investigate fatigue effects? -->

<!-- - but these problems don't seem to play a role in long RSI, so findings there are still valid.  -->
<!-- **Error rate long/short is not an issue for "orienting account"** -->

<!-- Check data for hints of classic/robust accounts being better? Can we find "global shifts" in parameters, or are differences C/E Location only "phases" and time specific to errors. -->

<!-- Check stop-signal task results, show that you could account for post-nogo effects using the anova approach -->
<!-- -describe theory -->
<!-- -describe what was done -->
<!-- - task, PES spec, model -->
<!-- - describe results -->
<!-- - say what that means theoretically, show what accounts it contributes with -->
<!-- - boundary separation last -->
<!-- - discuss potential reasons for this -->
<!-- - discuss how Damaso 2022 found same thing (Lex Task, with Confidence ratings, huge RSI) -->
<!-- - Might be due to confounding problems -->
<!-- - Might be due to errors being post-NoGo - that would only show in "robust" comparison -->
<!-- 	- Find some evidence of post-Nogo effects? -->
<!-- - Might be due to errors being post-response - that would only show in "classic" condition -->
<!-- 	- Find some evidence of post-inhibition response? -->
<!-- - Compare post-NoGo to post-Go (C-1 -- C+1) to fix first or C+1 -- C-1 -->
<!-- - Ultimately, you cannot separate a response following an error from also following a failed inhibition trial -->
<!-- - Maybe compare effects of Go-errors? Don't have any go-errors (practically) that aren't NA responses -->

<!-- might be due to real decreases in boundary separation -->
<!-- - talk about error types -->
<!-- - talk about post-error recklessness -->
<!-- 	- Damaso et al (2020) say post-error recklessness, lending to post-error speeding is found mainly after evidence-quality errors -->


<!-- ## further research -->
<!-- Better task! This one allowed the location model, but it's also shit -->
<!-- Investigate group-level sd in E-1 compared to E+1, more variance following an error? -> individual differences in error processing. Tentative suggestions at best -->
<!-- Just talk about the influence of tasks, error types and response instructions more generally. It seems to make a huge difference! -->

<!-- Speed and accuracy instruction seems to matter (Damaso, 2022), maybe take a look at whether individuals preferred speed/accuracy? Or manipulate that in later research -->

<!-- ## Limitations -->
<!-- Discuss model fit here -->
<!-- Discuss the paper suggesting LBA modelling here -->
<!-- - Damaso et al. suggest DDM bad because it doesn't separate error and correct responses -> bring up the error rate in trials analysed.  -->
<!-- - max error rate in my experiment is 5% - 480/7200 trials errors. (E+1) -->
<!-- Error types: Do I find evidence for response quality errors? Percent of error responses to nogo-trials slower than correct responses? Somewhat pointless because of inhibition. -->


<!-- Maybe say: Effect of error on rt not generalizable to "errors" in general but more specific to inhibition errors in nogo trials. Should be on the safe side with that. -->

<!-- Effect of error should depend on a) requirements of task and b) the process that failed -->
<!-- - maybe find some evidence of that -->