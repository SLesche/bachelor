# Discussion
The present study aimed to expand the study of cognitive processes underlying post-error slowing (PES) with the help of Drift Diffusion Modeling (DDM) and a systematic manipulation of the response-stimulus interval (RSI). We tried to investigate differences in the effect of error trials on response behavior depending on the time that has passed since the error has occurred. When the RSI is short (200ms) we expect mostly maladaptive processes to affect post-error behavior, leading to decreases in attention reflected by lower drift rates. Is the RSI long on the other hand, maladaptive processes should have dissipated and adaptive changes in behavior such as increased response caution reflected in higher boundary separation should account for post-error slowing.
 
Due to the models inability to accurately fit the response time distribution on post-error trials in the short RSI condition, parameters estimated in this condition are not interpretable. This prevents us from gaining any insight into differential effects of errors on post-error behavior depending on the RSI. 

Issues with fitting the data most likely stem from the 800ms response time limit censoring responses of participants. A considerable proportion of responses (`r 100*proportion_omissions_location %>% filter(rsi == 0.2, location == "E+1") %>% pull(freq)`%) were coded as omissions because participants did not respond before the deadline. This results in the steep drop after the 800ms deadline found when investigating this conditions' response time distribution (see Figure \@ref(fig:discussion-ep1-short-plot-rt-dist)). The basic DDM is not able to account for this censoring of response time distributions. Recently, researchers were able to extend the LBA model to accurately account for omissions of this type [@damaso2021]. Extending the DDM in this way may result in it being able to fit the present empirical data and will allow us to test our initial hypothesis by comparing parameter changes in long RSI to changes in short RSI. Developing and testing this extension would however exceed the scope of this body of work and will be relegated to future endeavors.

```{r discussion-ep1-short-plot-rt-dist, cache = TRUE, warning = FALSE, fig.cap='Reponse time distribution on post-error trials in the short rsi conditions. All responses shorter than 150ms were excluded, the next trial began after 800ms after stimulus presentation.'}
rt_dist_ep1_short <- diffusion_data_location %>% 
  filter(location == "E+1", rsi == "short") %>% 
  ggplot(
    aes(x = rt)
  )+
  geom_histogram(bins = 55)+
  xlim(0, 1)+
  labs(
    x = "Response time (in s)",
    y = "count"
  )+
  papaja::theme_apa()

rt_dist_ep1_short
```

Parameters obtained from data originating in the long RSI condition do not show this extreme misfit to the data. We expected to find evidence of adaptive processes at long RSI resulting in increased boundary separation. Maladaptive processes leading to attentional deficits following errors should lead to decreases in drift rate, which we mostly expected at short RSI. This decrease in drift rate should be smaller in long RSI.

In line with previous research [@dutilh2013; @purcell2016; @schiffler2017] we found post-error decreases in drift rate, even in the long RSI. Maladaptive processes such decreased attention as posed by the orientating account [@notebaert2009] or reduced availability of central cognitive processing capabilities posited by @dudschig2009 seem to influence post-error behavior even after 1000ms.

All those previous studies did however observe increases in boundary separation following errors. These findings supported the idea that errors lead to some attentional deficit that is  offset by increases in caution [@schiffler2017]. In our data, we failed to find any evidence for increases in boundary separation following errors. Decreases in boundary separation are not predicted by any accounts of PES and were only found in one other study [@damaso2022]. @damaso2022 found decreases in caution applying an LBA model to data obtained by @osth2017 but observed no decreases in boundary separation when fitting a DDM to the same data. Our results and those of @damaso2022 are however inconsistent with most prior research [@dutilh2012testing; @dutilh2013; @purcell2016; @schiffler2017] and require explanation.

One possible explanation might stem from problems in study design. The chosen task is extremely simple and does not require much evidence accumulation to take place. As such, accuracies in Go trials are very high. One might argue that participants do not accumulate evidence towards one of the decision boundaries "X" or "Y" but rather accumulate evidence towards "responding" or "not responding". This would violate the assumptions of the model specification employed here and calls in question the results of fitting this model.

Additionally, as the task is mostly consistent of Go trials and the RSI is constant in within a block, anticipation effects may play a large role. Participants anticipate having to give a Go response and begin their response process even before the trial has begun. The full DDM may be able to somewhat compensate for this issue by introducing variability of the starting point as a new parameter and thus allowing for some trials to have starting points extremely close to a response boundary. High impact of anticipation on responses should however lead to larger error rates in NoGo trials. Participants of our study had better performance than participants in previous studies employing similar versions of this task [@hester2007]. We also observed good fit of predicted response time distributions even in conditions with anticipation-induced "bumps", leading us to believe that the impact of anticipation was at least somewhat captured by our model.

Another potential issue is the confound of post-error and post-nogo behavior. All post-error trials are simultaneously post-NoGo trials whereas pre-error trials are post-Go trials. A potential solution to this problem is to not only consider the impact of errors by investigating the difference $\overline{RT_{E+1}} - \overline{RT_{E-1}}$ but also use the difference $\overline{RT_{C+1}} - \overline{RT_{C-1}}$ as a baseline to partial out post-NoGo effects. Falsely responding to a NoGo trial suggests that participants did not correctly process the NoGo trial. They also don't experience the long trial duration that accompanies a correct response inhibition in a NoGo trial. We therefore argue that responses following errors in NoGo trials are not impacted by "NoGo-effects" in the same way that responses following correct NoGo trials are. This leads us to advise against controlling for post-NoGo effects using differences in trials surrounding correct responses to NoGo trials. Due to this flaw in the design, we are not able to fully rule out impacts of NoGo-trials on post-error behavior.


in terms of reflecting true behavior:
- in our study boundary separation may not be the only predictor of caution!
- damaso et al said two different error types, their errors mostly evidence quality, where bs doesnt correlate with accuracy. This is not the case here
- this study has mostly response-speed errors, so increasing bs is the best way 
- post-error recklessness, this may be due to study,
in terms of stop-signal task: When stop-signal is more likely, bs goes up. "Stop signal" here is really unlikely following a NoGo trial, so bs might decrease to get faster rt.
Bias is best indicator of this!   

We did not find any evidence of adaptive post-error processes leading to increased caution but rather attribute our results to increased post-error recklessness following errors. Sadly we were not able to test our hypothesis regarding differential effects in the two conditions due to a flaw in study design. This can be accounted for in future modelling. Problems that cannot be fixed however is the compound of NoGo and stuff, We would thus suggest for future research:

A strength of this study is the ability to discern pre-correct from pre-error trials, this allows us to find effects even prior to error commission. **add this to results** Pre-error speeding is due to decreased boundary separation prior to error trials. This is more pronounced in long RSI because it is not accompanied by changes in bias or non-decision time parameters pre-error.

**maybe say something about robust vs. classical pes here?**
We observed global shifts in performance that may effects results obtained in classical condition. But the full location model shows pre-error speeding differences that will taint the results of robust measures. Both have their problems. Even though pre-error speeding is much more impactful in the long RSI condition, differences in DDM parameters reveal a more complicated structure in short RSI.

Problematic as it may be, this study gives further warning against interpreting PES as a unitary construct. Many factors are involved when determining the size and functionality of slower RT following errors. These include: RSI, Task, Feedback, Participants, Error type

Future endeavors could do the following things better: better task, make sure no omissions...

**possibly say something about robust vs. classical accounts here**


<!-- NOTES ------------- -->

Report results but strongly advise against using them, explain them in terms of post-error frustration or post-error recklessnes
here is why
bad fit to empricial data
- truncated responses at short e+1 mostly (40% missing data)
- anticipation effects
- multiple post-error processes
- ideal of "more cautios behavior" might not be real. Affective components always play a role
- sampling (non-omitted) trials, mostly trials that have lower boundary separation

- participants not working on the task properly? accuracy from nogo trials suggests differently

- run a model with median split trial number to investigate fatigue effects?

- but these problems don't seem to play a role in long RSI, so findings there are still valid. 
**Error rate long/short is not an issue for "orienting account"**

Check data for hints of classic/robust accounts being better? Can we find "global shifts" in parameters, or are differences C/E Location only "phases" and time specific to errors.

Check stop-signal task results, show that you could account for post-nogo effects using the anova approach
<!-- -describe theory -->
<!-- -describe what was done -->
<!-- - task, PES spec, model -->
<!-- - describe results -->
<!-- - say what that means theoretically, show what accounts it contributes with -->
<!-- - boundary separation last -->
<!-- - discuss potential reasons for this -->
<!-- - discuss how Damaso 2022 found same thing (Lex Task, with Confidence ratings, huge RSI) -->
<!-- - Might be due to confounding problems -->
<!-- - Might be due to errors being post-NoGo - that would only show in "robust" comparison -->
<!-- 	- Find some evidence of post-Nogo effects? -->
<!-- - Might be due to errors being post-response - that would only show in "classic" condition -->
<!-- 	- Find some evidence of post-inhibition response? -->
<!-- - Compare post-NoGo to post-Go (C-1 -- C+1) to fix first or C+1 -- C-1 -->
<!-- - Ultimately, you cannot separate a response following an error from also following a failed inhibition trial -->
<!-- - Maybe compare effects of Go-errors? Don't have any go-errors (practically) that aren't NA responses -->

<!-- might be due to real decreases in boundary separation -->
<!-- - talk about error types -->
<!-- - talk about post-error recklessness -->
<!-- 	- Damaso et al (2020) say post-error recklessness, lending to post-error speeding is found mainly after evidence-quality errors -->


<!-- ## further research -->
<!-- Better task! This one allowed the location model, but it's also shit -->
<!-- Investigate group-level sd in E-1 compared to E+1, more variance following an error? -> individual differences in error processing. Tentative suggestions at best -->
<!-- Just talk about the influence of tasks, error types and response instructions more generally. It seems to make a huge difference! -->

<!-- Speed and accuracy instruction seems to matter (Damaso, 2022), maybe take a look at whether individuals preferred speed/accuracy? Or manipulate that in later research -->

<!-- ## Limitations -->
<!-- Discuss model fit here -->
<!-- Discuss the paper suggesting LBA modelling here -->
<!-- - Damaso et al. suggest DDM bad because it doesn't separate error and correct responses -> bring up the error rate in trials analysed.  -->
<!-- - max error rate in my experiment is 5% - 480/7200 trials errors. (E+1) -->
<!-- Error types: Do I find evidence for response quality errors? Percent of error responses to nogo-trials slower than correct responses? Somewhat pointless because of inhibition. -->


<!-- Maybe say: Effect of error on rt not generalizable to "errors" in general but more specific to inhibition errors in nogo trials. Should be on the safe side with that. -->

<!-- Effect of error should depend on a) requirements of task and b) the process that failed -->
<!-- - maybe find some evidence of that -->